{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from beir.retrieval import models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# === Set your hyperparameters here ===\n",
    "\n",
    "# Choose the dataset to use from [\"en\", \"de\"]\n",
    "lang = \"en\"\n",
    "assert lang in [\"en\", \"de\"]\n",
    "\n",
    "# Set to true if splitting the survey variables with sub-questions into separate corpus items\n",
    "sep_answers = False\n",
    "assert isinstance(sep_answers, bool)\n",
    "\n",
    "# The string used to join survey variable information\n",
    "join_str = \" [UNK] \"\n",
    "assert isinstance(join_str, str)\n",
    "\n",
    "# Evaluate each of the top k values\n",
    "k_values = [1, 3, 5, 10, 100]\n",
    "assert isinstance(k_values, list)\n",
    "all(isinstance(e, int) for e in k_values)\n",
    "\n",
    "# Embedding similarity function from [\"cos_sim\", \"dot_score\"]\n",
    "score_function = \"cos_sim\"\n",
    "assert score_function in [\"cos_sim\", \"dot_score\"]\n",
    "\n",
    "# Any sentence-transformers (https://www.sbert.net/docs/pretrained_models.html)\n",
    "# or HuggingFace model (https://huggingface.co/models) works \n",
    "pretrained_models = [\"all-MiniLM-L6-v2\", \"paraphrase-MiniLM-L3-v2\", \"all-mpnet-base-v2\", \"paraphrase-multilingual-mpnet-base-v2\"]\n",
    "# pretrained_models = [\"paraphrase-MiniLM-L3-v2\", \"paraphrase-multilingual-mpnet-base-v2\", \"multi-qa-distilbert-cos-v1\", \"paraphrase-multilingual-MiniLM-L12-v2\"]\n",
    "\n",
    "# Set batch size for inference\n",
    "batch_size = 16\n",
    "assert isinstance(batch_size, int)\n",
    "\n",
    "# ================ End ================"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def split_variables(row):\n",
    "    if row not in [\"No\", \"NoSkip\"]:\n",
    "        return [tuple(x.split('-')) for x in row.replace('[','').replace(']','').split(',')]\n",
    "    else:\n",
    "        return [row]\n",
    "    \n",
    "def get_variables(row):\n",
    "    return [x[0] for x in row.variables]\n",
    "\n",
    "def make_label(row, answer, join_str):\n",
    "    v_id = row[\"id\"] if row[\"id\"] else \"\"\n",
    "    v_label = row[\"label\"] if row[\"label\"] else \"\"\n",
    "    v_topic = row[\"topic\"] if row[\"topic\"] else \"\"\n",
    "    v_question = row[\"question\"] if row[\"question\"] else \"\"\n",
    "    v_answer = answer if answer else \"\"\n",
    "\n",
    "    label = join_str.join([v_label, v_topic, v_question, v_answer])\n",
    "    \n",
    "    return label\n",
    "\n",
    "def get_labels(df, sep_answers=False, join_str=\"[UNK]\"):\n",
    "    ids = []\n",
    "    labels = []\n",
    "\n",
    "    df.fillna(\"\", inplace=True)\n",
    "\n",
    "    for i,row in df.iterrows():\n",
    "\n",
    "        if sep_answers:  # split survey variable answers into separate corpus items\n",
    "            answers = row[\"answer\"].split(\";\")\n",
    "        else:  # do not split survey variable answers\n",
    "            answers = [row[\"answer\"] if row[\"answer\"] else \"\"]\n",
    "            \n",
    "        for v_answer in answers:\n",
    "            v_id = row[\"id\"] if row[\"id\"] else \"\"\n",
    "            label = make_label(row, v_answer, join_str)\n",
    "            ids.append(v_id)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return ids, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load raw data\n",
    "\n",
    "data_path = '../../data/trial/test/en.tsv' if lang != \"de\" else '../../data/trial/test/de.tsv'\n",
    "variables_path = '../../data/trial/vocabulary/en.tsv' if lang != \"de\" else '../../data/trial/vocabulary/de.tsv'\n",
    "\n",
    "data_df = pd.read_csv(data_path, sep=\"\\t\")\n",
    "data_df.rename(columns={\"is_variable\": \"label\"}, inplace=True)\n",
    "data_df[\"variables\"] = data_df.variable.apply(lambda x: split_variables(x))\n",
    "\n",
    "variable_df = pd.read_csv(variables_path, sep=\"\\t\")\n",
    "\n",
    "ids, labels = get_labels(variable_df, sep_answers, join_str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def make_beir_data(data_df, beir_data_dir):\n",
    "    queries = {}\n",
    "    qrels = {}\n",
    "    for i,row in data_df.iterrows():\n",
    "        queries[str(i)] = row.text\n",
    "        rel_labels = []\n",
    "        if row.variable not in [\"No\", \"NoSkip\"]:\n",
    "            rel_labels = [\"v\"+x for x in get_variables(row)]\n",
    "        qrels[str(i)] = rel_labels        \n",
    "\n",
    "    corpus = {}\n",
    "    for i,label in enumerate(labels):\n",
    "        corpus[ids[i]] = label\n",
    "\n",
    "    beir_qrels_dir = os.path.join(beir_data_dir, \"qrels\")\n",
    "    if not os.path.exists(beir_qrels_dir):\n",
    "        os.makedirs(beir_qrels_dir)\n",
    "\n",
    "    queries_beir = []\n",
    "    for k,v in queries.items():\n",
    "        queries_beir.append({\"_id\": k, \"text\": v})\n",
    "\n",
    "    corpus_beir = []\n",
    "    for k,v in corpus.items():\n",
    "        corpus_beir.append({\"_id\": k, \"title\": \"\", \"text\": v})\n",
    "        # corpus_beir.append({\"_id\": k, \"title\": f\"Doc_{str(k)}\", \"text\": v})\n",
    "\n",
    "    qrels_beir = {\"query-id\": [], \"corpus-id\": [], \"score\": []}\n",
    "\n",
    "    for k,vals in qrels.items():\n",
    "        for v in vals:\n",
    "            qrels_beir[\"query-id\"].append(k)\n",
    "            qrels_beir[\"corpus-id\"].append(v)\n",
    "            qrels_beir[\"score\"].append(1)\n",
    "\n",
    "    df = pd.DataFrame.from_records(qrels_beir)\n",
    "    df[[\"query-id\", \"corpus-id\", \"score\"]].to_csv(os.path.join(beir_data_dir, \"qrels\", \"all.tsv\"), index=False, sep=\"\\t\")\n",
    "\n",
    "    with jsonlines.open(os.path.join(beir_data_dir, \"queries.jsonl\"), \"w\") as writer:\n",
    "        writer.write_all(queries_beir)\n",
    "\n",
    "    with jsonlines.open(os.path.join(beir_data_dir, \"corpus.jsonl\"), \"w\") as writer:\n",
    "        writer.write_all(corpus_beir)\n",
    "    \n",
    "    return queries_beir, corpus_beir, qrels_beir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Load data in BEIR format\n",
    "\n",
    "beir_data_dir = f\"../../data/trial/beir_data/{lang}\"\n",
    "_queries_beir, _corpus_beir, _qrels_beir = make_beir_data(data_df, beir_data_dir)\n",
    "\n",
    "corpus, queries, qrels = GenericDataLoader(data_folder=beir_data_dir).load(split=\"all\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 182/182 [00:00<00:00, 245612.40it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model_results = {}\n",
    "\n",
    "for model_name in pretrained_models:\n",
    "    # Initialize retriever model\n",
    "    model = DRES(models.SentenceBERT(model_name), batch_size=batch_size)\n",
    "    retriever = EvaluateRetrieval(model, score_function=score_function, k_values=k_values)\n",
    "\n",
    "    # Evaluate model using multiple metrics\n",
    "    results = retriever.retrieve(corpus, queries, return_sorted=True)\n",
    "    model_results[model_name] = results"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 110.49it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 175.16it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 299.17it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 238.23it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 79.82it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 58.47it/s]\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 65.71it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:00<00:00, 56.53it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_sorted_counted_results(model_results):\n",
    "\n",
    "    all_counts = {}\n",
    "    \n",
    "    corpus_ids = list(model_results[list(model_results.keys())[0]].keys())\n",
    "\n",
    "    for corpus_id in corpus_ids:\n",
    "\n",
    "        all_vars = []\n",
    "        all_var_scores = defaultdict(list)\n",
    "        for model_name,res in model_results.items():\n",
    "            assert (corpus_id in res)\n",
    "\n",
    "            all_vars.extend(res[corpus_id])\n",
    "\n",
    "            for var,score in res[corpus_id].items():\n",
    "                all_var_scores[var].append(score)\n",
    "        \n",
    "        counts = Counter(all_vars)\n",
    "\n",
    "        avg_scores = {}\n",
    "        for var,scores in all_var_scores.items():\n",
    "            avg_scores[var] = sum(scores)/len(scores)\n",
    "        \n",
    "        score_counts = {}\n",
    "        for c,v in counts.items():\n",
    "            score_counts[c] = (v, avg_scores[c])\n",
    "\n",
    "        # all_counts[corpus_id] = counts       \n",
    "        # all_avg_scores[corpus_id] = avg_scores\n",
    "        all_counts[corpus_id] = score_counts\n",
    "    \n",
    "    return all_counts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# all_scores = get_sorted_counted_results(model_results)\n",
    "# sorted_counts = sorted(counts.items(), key=lambda x: x[::-1], reverse=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_pooled_results(model_results):\n",
    "\n",
    "    orig_results_length = len(next(iter(next(iter(model_results.values())).values())))\n",
    "\n",
    "    all_scores = get_sorted_counted_results(model_results)\n",
    "    all_sorted_counts = {}\n",
    "\n",
    "    for corpus_id,counts in all_scores.items():\n",
    "        sorted_counts = sorted(counts.items(), key=lambda x: x[::-1], reverse=True)\n",
    "\n",
    "        weighted_counts = {}\n",
    "        for c,(count,score) in sorted_counts:\n",
    "            weighted_counts[c] = (count*(score/len(model_results)))\n",
    "        \n",
    "        filtered_counts_tuple = sorted(weighted_counts.items(), key=lambda x: x[1], reverse=True)[:orig_results_length]\n",
    "        filtered_keys = [k[0] for k in filtered_counts_tuple]\n",
    "        filtered_counts = {k:v for k,v in weighted_counts.items() if k in filtered_keys}\n",
    "\n",
    "        all_sorted_counts[corpus_id] = filtered_counts\n",
    "    \n",
    "    return all_sorted_counts"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "pooled_results = get_pooled_results(model_results)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Save files\n",
    "\n",
    "import json\n",
    "\n",
    "with open(f\"./qrels_dense_{lang}.json\", \"w\") as fp:\n",
    "    json.dump(qrels, fp)\n",
    "\n",
    "with open(f\"./run_dense_{lang}.json\", \"w\") as fp:\n",
    "    json.dump(results, fp)\n",
    "\n",
    "with open(f\"./run_dense_pooled_{lang}.json\", \"w\") as fp:\n",
    "    json.dump(pooled_results, fp)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "d9c75a10141fb9765b6dd95572dd5e09c2b8655ce454a09c7fa750dc40f7865b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}