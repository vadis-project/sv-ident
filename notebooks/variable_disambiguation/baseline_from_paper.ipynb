{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import copy\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytrec_eval\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from beir.retrieval import models\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from beir.retrieval.search.lexical import BM25Search as BM25\n",
    "from beir.retrieval.search.sparse import SparseSearch\n",
    "\n",
    "\n",
    "PYTREC_METRIC_MAPPING = {\"map\": \"map_cut\", \"rprec\": \"Rprec\", \"p\": \"P\", \"r\": \"recall\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables(path):\n",
    "    assert \".json\" == path[-5:]\n",
    "    variables = {}\n",
    "    with open(path, \"r\") as fp:\n",
    "        variables = json.load(fp)\n",
    "    return variables\n",
    "\n",
    "\n",
    "def make_label(meta, sep=\" [UNK] \"):\n",
    "    label = []\n",
    "    for k,v in meta.items():\n",
    "        if isinstance(v, list):\n",
    "            v = sep.join(v)\n",
    "        if \";\" in v:\n",
    "            v = sep.join(v.split(\";\"))\n",
    "        label.append(v)\n",
    "    return sep.join(label)\n",
    "\n",
    "\n",
    "def get_instance_variables(df, variables):\n",
    "    instance_variables = {k:make_label(v) for rd in df.iloc[0][\"research_data\"].split(\";\") for k,v in variables[rd].items()}\n",
    "    id_mapping = {k:str(i) for i,(k,_) in enumerate(instance_variables.items())}\n",
    "    return instance_variables, id_mapping\n",
    "\n",
    "\n",
    "def get_qrels(df, mapping):\n",
    "    qrels = {\"query-id\": [], \"corpus-id\": [], \"score\": [], \"uuid\": []}\n",
    "    qrels_with_unk = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i]\n",
    "        uuid = row[\"uuid\"]\n",
    "        is_variable = row[\"is_variable\"]\n",
    "\n",
    "        if is_variable == 1:\n",
    "            vs = row[\"variable\"].split(\";\") if \";\" in row[\"variable\"] else [row[\"variable\"]]\n",
    "            for v in vs:\n",
    "                if v == \"unk\":\n",
    "                    qrels_with_unk.append(i)\n",
    "                    continue\n",
    "                elif v not in mapping:\n",
    "                    print(f\"{v} not in mapping!\")\n",
    "                    continue\n",
    "                \n",
    "                _id = mapping[v]\n",
    "                qrels[\"query-id\"].append(str(i))\n",
    "                qrels[\"corpus-id\"].append(str(_id))\n",
    "                qrels[\"score\"].append(1)\n",
    "                qrels[\"uuid\"].append(uuid)\n",
    "    return qrels\n",
    "\n",
    "\n",
    "def get_corpus(df, ivariables, mapping):\n",
    "    corpus = []\n",
    "\n",
    "    for uuid,v in ivariables.items():\n",
    "        _id = mapping[uuid]\n",
    "        instance = {\"_id\": _id, \"title\": \"\", \"text\": v, \"uuid\": uuid}\n",
    "        corpus.append(instance)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def get_queries(df):\n",
    "    queries = []\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        row = df.iloc[i]\n",
    "        uuid = row[\"uuid\"]\n",
    "        text = row[\"sentence\"]\n",
    "\n",
    "        query = {\"_id\": str(i), \"text\": text, \"uuid\": uuid}\n",
    "        queries.append(query)\n",
    "    return queries\n",
    "\n",
    "\n",
    "def save_jsonl(data, path):\n",
    "    with jsonlines.open(path, \"w\") as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "def save_files(queries, corpus, qrels, data_dir):\n",
    "    qrels_dir = os.path.join(data_dir, \"qrels\")\n",
    "    if not os.path.exists(qrels_dir):\n",
    "        os.makedirs(qrels_dir)\n",
    "\n",
    "    queries_path = os.path.join(data_dir, \"queries.jsonl\")\n",
    "    save_jsonl(queries, queries_path)\n",
    "    corpus_path = os.path.join(data_dir, \"corpus.jsonl\")\n",
    "    save_jsonl(corpus, corpus_path)\n",
    "\n",
    "    qrels_path = os.path.join(qrels_dir, \"all.tsv\")\n",
    "    qrels_df = pd.DataFrame(qrels)\n",
    "    qrels_df.to_csv(qrels_path, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataLoader:\n",
    "    \n",
    "    def __init__(self, data_folder: str = None, prefix: str = None, corpus_file: str = \"corpus.jsonl\", query_file: str = \"queries.jsonl\", \n",
    "                 qrels_folder: str = \"qrels\", qrels_file: str = \"\"):\n",
    "        self.corpus = {}\n",
    "        self.queries = {}\n",
    "        self.qrels = {}\n",
    "\n",
    "        self.query_corpus_mapping = {}\n",
    "        \n",
    "        if prefix:\n",
    "            query_file = prefix + \"-\" + query_file\n",
    "            qrels_folder = prefix + \"-\" + qrels_folder\n",
    "\n",
    "        self.corpus_file = os.path.join(data_folder, corpus_file) if data_folder else corpus_file\n",
    "        self.query_file = os.path.join(data_folder, query_file) if data_folder else query_file\n",
    "        self.qrels_folder = os.path.join(data_folder, qrels_folder) if data_folder else None\n",
    "        self.qrels_file = qrels_file\n",
    "    \n",
    "    @staticmethod\n",
    "    def check(fIn: str, ext: str):\n",
    "        if not os.path.exists(fIn):\n",
    "            raise ValueError(\"File {} not present! Please provide accurate file.\".format(fIn))\n",
    "        \n",
    "        if not fIn.endswith(ext):\n",
    "            raise ValueError(\"File {} must be present with extension {}\".format(fIn, ext))\n",
    "\n",
    "    def load_custom(self) -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:\n",
    "\n",
    "        self.check(fIn=self.corpus_file, ext=\"jsonl\")\n",
    "        self.check(fIn=self.query_file, ext=\"jsonl\")\n",
    "        self.check(fIn=self.qrels_file, ext=\"tsv\")\n",
    "        \n",
    "        if not len(self.corpus):\n",
    "            self._load_corpus()\n",
    "        \n",
    "        if not len(self.queries):\n",
    "            self._load_queries()\n",
    "        \n",
    "        if os.path.exists(self.qrels_file):\n",
    "            self._load_qrels()\n",
    "            self.queries = {qid: self.queries[qid] for qid in self.qrels}\n",
    "        \n",
    "        return self.corpus, self.queries, self.qrels\n",
    "\n",
    "    def load(self, split=\"test\") -> Tuple[Dict[str, Dict[str, str]], Dict[str, str], Dict[str, Dict[str, int]]]:\n",
    "        \n",
    "        self.qrels_file = os.path.join(self.qrels_folder, split + \".tsv\")\n",
    "        self.check(fIn=self.corpus_file, ext=\"jsonl\")\n",
    "        self.check(fIn=self.query_file, ext=\"jsonl\")\n",
    "        self.check(fIn=self.qrels_file, ext=\"tsv\")\n",
    "        \n",
    "        if not len(self.corpus):\n",
    "            self._load_corpus()\n",
    "        \n",
    "        if not len(self.queries):\n",
    "            self._load_queries()\n",
    "        \n",
    "        if os.path.exists(self.qrels_file):\n",
    "            self._load_qrels()\n",
    "            self.queries = {qid: self.queries[qid] for qid in self.qrels}\n",
    "\n",
    "        # Re-format\n",
    "        self.corpus = {v[\"uuid\"]: {\"text\": v[\"text\"], \"title\": v[\"title\"], \"_id\": k} for k,v in self.corpus.items()}\n",
    "        \n",
    "        return self.corpus, self.queries, self.qrels\n",
    "    \n",
    "    def load_corpus(self) -> Dict[str, Dict[str, str]]:\n",
    "        \n",
    "        self.check(fIn=self.corpus_file, ext=\"jsonl\")\n",
    "\n",
    "        if not len(self.corpus):\n",
    "            self._load_corpus()\n",
    "\n",
    "        return self.corpus\n",
    "    \n",
    "    def _load_corpus(self):\n",
    "    \n",
    "        with open(self.corpus_file, encoding='utf8') as fIn:\n",
    "            for line in fIn:\n",
    "                line = json.loads(line)\n",
    "                self.corpus[line.get(\"_id\")] = {\n",
    "                    \"text\": line.get(\"text\"),\n",
    "                    \"title\": line.get(\"title\"),\n",
    "                    \"uuid\": line.get(\"uuid\"),\n",
    "                }\n",
    "    \n",
    "    def _load_queries(self):\n",
    "        \n",
    "        with open(self.query_file, encoding='utf8') as fIn:\n",
    "            for line in fIn:\n",
    "                line = json.loads(line)\n",
    "                self.queries[line.get(\"uuid\")] = line.get(\"text\")\n",
    "        \n",
    "    def _load_qrels(self):\n",
    "        \n",
    "        reader = csv.reader(open(self.qrels_file, encoding=\"utf-8\"), \n",
    "                            delimiter=\"\\t\", quoting=csv.QUOTE_MINIMAL)\n",
    "        next(reader)\n",
    "        \n",
    "        for id, row in enumerate(reader):\n",
    "            query_id, corpus_id, score = row[3], row[1], int(row[2])\n",
    "            \n",
    "            if query_id not in self.qrels:\n",
    "                self.qrels[query_id] = {self.corpus[corpus_id]['uuid']: score}\n",
    "            else:\n",
    "                self.qrels[query_id][self.corpus[corpus_id]['uuid']] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDatasetDataLoader:\n",
    "    def __init__(self, datasets, batch_size_pairs, batch_size_triplets=None, dataset_size_temp=-1):\n",
    "        self.allow_swap = True\n",
    "        self.batch_size_pairs = batch_size_pairs\n",
    "        self.batch_size_triplets = batch_size_pairs if batch_size_triplets is None else batch_size_triplets\n",
    "\n",
    "        # Compute dataset weights\n",
    "        self.dataset_lengths = list(map(len, datasets))\n",
    "        self.dataset_lengths_sum = sum(self.dataset_lengths)\n",
    "\n",
    "        weights = []\n",
    "        if dataset_size_temp > 0:  # Scale probability with dataset size\n",
    "            for dataset in datasets:\n",
    "                prob = len(dataset) / self.dataset_lengths_sum\n",
    "                weights.append(max(1, int(math.pow(prob, 1 / dataset_size_temp) * 1000)))\n",
    "        else:  # Equal weighting of all datasets\n",
    "            weights = [100] * len(datasets)\n",
    "\n",
    "        self.dataset_idx = []\n",
    "        self.dataset_idx_pointer = 0\n",
    "\n",
    "        for idx, weight in enumerate(weights):\n",
    "            self.dataset_idx.extend([idx] * weight)\n",
    "        random.shuffle(self.dataset_idx)\n",
    "\n",
    "        self.datasets = []\n",
    "        for dataset in datasets:\n",
    "            random.shuffle(dataset)\n",
    "            self.datasets.append({\n",
    "                'elements': dataset,\n",
    "                'pointer': 0,\n",
    "            })\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(int(self.__len__())):\n",
    "            # Select dataset\n",
    "            if self.dataset_idx_pointer >= len(self.dataset_idx):\n",
    "                self.dataset_idx_pointer = 0\n",
    "                random.shuffle(self.dataset_idx)\n",
    "\n",
    "            dataset_idx = self.dataset_idx[self.dataset_idx_pointer]\n",
    "            self.dataset_idx_pointer += 1\n",
    "\n",
    "            # Select batch from this dataset\n",
    "            dataset = self.datasets[dataset_idx]\n",
    "            batch_size = self.batch_size_pairs if len(dataset['elements'][0].texts) == 2 else self.batch_size_triplets\n",
    "\n",
    "            batch = []\n",
    "            texts_in_batch = set()\n",
    "            guid_in_batch = set()\n",
    "            while len(batch) < batch_size:\n",
    "                example = dataset['elements'][dataset['pointer']]\n",
    "\n",
    "                valid_example = True\n",
    "                # First check if one of the texts in already in the batch\n",
    "                for text in example.texts:\n",
    "                    text_norm = text.strip().lower()\n",
    "                    if text_norm in texts_in_batch:\n",
    "                        valid_example = False\n",
    "\n",
    "                    texts_in_batch.add(text_norm)\n",
    "\n",
    "                # If the example has a guid, check if guid is in batch\n",
    "                if example.guid is not None:\n",
    "                    valid_example = valid_example and example.guid not in guid_in_batch\n",
    "                    guid_in_batch.add(example.guid)\n",
    "\n",
    "\n",
    "                if valid_example:\n",
    "                    if self.allow_swap and random.random() > 0.5:\n",
    "                        example.texts[0], example.texts[1] = example.texts[1], example.texts[0]\n",
    "\n",
    "                    batch.append(example)\n",
    "\n",
    "                dataset['pointer'] += 1\n",
    "                if dataset['pointer'] >= len(dataset['elements']):\n",
    "                    dataset['pointer'] = 0\n",
    "                    random.shuffle(dataset['elements'])\n",
    "\n",
    "            yield self.collate_fn(batch) if self.collate_fn is not None else batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(self.dataset_lengths_sum / self.batch_size_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBERT:\n",
    "    def __init__(self, model_path: Union[str, Tuple] = None, sep: str = \" \", **kwargs):\n",
    "        self.sep = sep\n",
    "        \n",
    "        if isinstance(model_path, str):\n",
    "            self.q_model = SentenceTransformer(model_path)\n",
    "            self.doc_model = self.q_model\n",
    "        \n",
    "        elif isinstance(model_path, tuple):\n",
    "            self.q_model = SentenceTransformer(model_path[0])\n",
    "            self.doc_model = SentenceTransformer(model_path[1])\n",
    "\n",
    "        elif isinstance(model_path, SentenceTransformer):\n",
    "            self.q_model = model_path\n",
    "            self.doc_model = self.q_model\n",
    "    \n",
    "    def encode_queries(self, queries: List[str], batch_size: int = 16, show_progress_bar: bool = False, **kwargs) -> Union[List[Tensor], np.ndarray, Tensor]:\n",
    "        self.q_model.show_progress_bar = False\n",
    "        return self.q_model.encode(queries, batch_size=batch_size, **kwargs)\n",
    "    \n",
    "    def encode_corpus(self, corpus: List[Dict[str, str]], batch_size: int = 8, show_progress_bar: bool = False, **kwargs) -> Union[List[Tensor], np.ndarray, Tensor]:\n",
    "        self.q_model.show_progress_bar = False\n",
    "        sentences = [(doc[\"title\"] + self.sep + doc[\"text\"]).strip() if \"title\" in doc else doc[\"text\"].strip() for doc in corpus]\n",
    "        return self.doc_model.encode(sentences, batch_size=batch_size, **kwargs)\n",
    "\n",
    "\n",
    "def get_mean_scores(all_scores):\n",
    "    scores = defaultdict(list)\n",
    "\n",
    "    for name,_scores in all_scores.items():\n",
    "        n = _scores[\"n\"]\n",
    "        for s in _scores[\"score\"].items():\n",
    "            scores[s[0]].extend([s[-1]]*n)\n",
    "\n",
    "    mean_scores = {}\n",
    "\n",
    "    for name,_scores in scores.items():\n",
    "        avg = sum(_scores)/len(_scores)\n",
    "        mean_scores[name] = avg\n",
    "\n",
    "    return mean_scores\n",
    "\n",
    "\n",
    "def get_metrics(metrics_str):\n",
    "    metrics = {}\n",
    "    for m in metrics_str:\n",
    "        parts = m.split(\"@\")\n",
    "        if len(parts) == 2:\n",
    "            name = \"\"\n",
    "            ks = None\n",
    "\n",
    "            try:\n",
    "                name, ks = parts[0].lower(), parts[1]\n",
    "            except:\n",
    "                print(f\"Invalid format for metric: {m}\")\n",
    "\n",
    "            if name in PYTREC_METRIC_MAPPING:\n",
    "                _metric = PYTREC_METRIC_MAPPING[name]+\".\"+ks\n",
    "                metrics[name] = _metric\n",
    "\n",
    "        elif len(parts) == 1:\n",
    "            name = m.lower()\n",
    "\n",
    "            if name in PYTREC_METRIC_MAPPING:\n",
    "                _metric = PYTREC_METRIC_MAPPING[name]\n",
    "                metrics[name] = _metric\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _evaluate(\n",
    "    qrels: Dict[str, Dict[str, int]], \n",
    "    results: Dict[str, Dict[str, float]], \n",
    "    measures: Dict[str, str]\n",
    "    ) -> Tuple[Dict[str, float], Dict[str, float], Dict[str, float], Dict[str, float]]:\n",
    "        \n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels, [k for _,k in measures.items()])\n",
    "        scores = evaluator.evaluate(results)\n",
    "\n",
    "        metrics = {_k:0.0 for _k,__ in scores[list(scores.keys())[0]].items()}\n",
    "\n",
    "        for _,query_scores in scores.items():\n",
    "            for score_name,score in query_scores.items():\n",
    "                metrics[score_name] += score\n",
    "\n",
    "        for k,v in metrics.items():\n",
    "            metrics[k] = round(v/len(scores),5)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def split_variables(row):\n",
    "    if row not in [\"No\", \"NoSkip\"]:\n",
    "        return [tuple(x.split('-')) for x in row.replace('[','').replace(']','').split(',')]\n",
    "    else:\n",
    "        return [row]\n",
    "    \n",
    "    \n",
    "def get_variables(row):\n",
    "    return [x[0] for x in row.variables]\n",
    "\n",
    "\n",
    "def drop_unk_variables(df):\n",
    "    new_rows = []\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        row = copy.deepcopy(df.iloc[i])\n",
    "        variables = row[\"variable\"].split(\";\") if isinstance(row[\"variable\"], str) else []\n",
    "        \n",
    "        vrds = \";\".join([v for v in variables if v != \"unk\"])\n",
    "        \n",
    "        if vrds:\n",
    "            row[\"variables\"] = vrds\n",
    "            new_rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "def make_label2(row, answer, join_str):\n",
    "    v_id = row[\"id\"] if row[\"id\"] else \"\"\n",
    "    v_label = row[\"label\"] if row[\"label\"] else \"\"\n",
    "    v_topic = row[\"topic\"] if row[\"topic\"] else \"\"\n",
    "    v_question = row[\"question\"] if row[\"question\"] else \"\"\n",
    "    v_answer = answer if answer else \"\"\n",
    "\n",
    "    label = join_str.join([v_label, v_topic, v_question, v_answer])\n",
    "    \n",
    "    return label\n",
    "\n",
    "\n",
    "def get_labels(df, sep_answers=False, join_str=\"[UNK]\"):\n",
    "    ids = []\n",
    "    labels = []\n",
    "\n",
    "    df.fillna(\"\", inplace=True)\n",
    "\n",
    "    for i,row in df.iterrows():\n",
    "\n",
    "        if sep_answers:  # split survey variable answers into separate corpus items\n",
    "            answers = row[\"answer\"].split(\";\")\n",
    "        else:  # do not split survey variable answers\n",
    "            answers = [row[\"answer\"] if row[\"answer\"] else \"\"]\n",
    "            \n",
    "        for v_answer in answers:\n",
    "            v_id = row[\"id\"] if row[\"id\"] else \"\"\n",
    "            label = make_label2(row, v_answer, join_str)\n",
    "            ids.append(v_id)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return ids, labels\n",
    "\n",
    "\n",
    "def make_beir_data(data_df, beir_data_dir, labels, ids):\n",
    "    queries = {}\n",
    "    qrels = {}\n",
    "    for i,row in data_df.iterrows():\n",
    "        queries[str(i)] = row.text\n",
    "        rel_labels = []\n",
    "        if row.variable not in [\"No\", \"NoSkip\"]:\n",
    "            rel_labels = [\"v\"+x for x in get_variables(row)]\n",
    "        qrels[str(i)] = rel_labels\n",
    "\n",
    "    corpus = {}\n",
    "    for i,label in enumerate(labels):\n",
    "        corpus[ids[i]] = label\n",
    "\n",
    "    beir_qrels_dir = os.path.join(beir_data_dir, \"qrels\")\n",
    "    if not os.path.exists(beir_qrels_dir):\n",
    "        os.makedirs(beir_qrels_dir)\n",
    "\n",
    "    queries_beir = []\n",
    "    for k,v in queries.items():\n",
    "        queries_beir.append({\"_id\": k, \"text\": v})\n",
    "\n",
    "    corpus_beir = []\n",
    "    for k,v in corpus.items():\n",
    "        corpus_beir.append({\"_id\": k, \"title\": \"\", \"text\": v})\n",
    "\n",
    "    qrels_beir = {\"query-id\": [], \"corpus-id\": [], \"score\": []}\n",
    "\n",
    "    for k,vals in qrels.items():\n",
    "        for v in vals:\n",
    "            qrels_beir[\"query-id\"].append(k)\n",
    "            qrels_beir[\"corpus-id\"].append(v)\n",
    "            qrels_beir[\"score\"].append(1)\n",
    "\n",
    "    df = pd.DataFrame.from_records(qrels_beir)\n",
    "    df[[\"query-id\", \"corpus-id\", \"score\"]].to_csv(os.path.join(beir_data_dir, \"qrels\", \"all.tsv\"), index=False, sep=\"\\t\")\n",
    "\n",
    "    with jsonlines.open(os.path.join(beir_data_dir, \"queries.jsonl\"), \"w\") as writer:\n",
    "        writer.write_all(queries_beir)\n",
    "\n",
    "    with jsonlines.open(os.path.join(beir_data_dir, \"corpus.jsonl\"), \"w\") as writer:\n",
    "        writer.write_all(corpus_beir)\n",
    "    \n",
    "    return queries_beir, corpus_beir, qrels_beir\n",
    "\n",
    "\n",
    "def load_retriever(model_path, batch_size, score_function, k_values, retriever_type=\"dense\", show_progress_bar=False):\n",
    "    if isinstance(model_path, str) or isinstance(model_path, SentenceTransformer) or model_path == \"BM25\":\n",
    "        if retriever_type == \"dense\":\n",
    "            model = DRES(SentenceBERT(model_path), batch_size=batch_size, show_progress_bar=show_progress_bar)\n",
    "        elif \"BM25-\" in retriever_type:\n",
    "            language = retriever_type.split(\"-\")[-1]\n",
    "            model = BM25(index_name=\"svident\", hostname=\"localhost\", language=language, initialize=True, number_of_shards=1)\n",
    "        elif \"sparse\":\n",
    "            model = SparseSearch(models.SPARTA(model_path), batch_size=batch_size)\n",
    "        else:\n",
    "            raise Exception(f\"Unknown retriever type: {retriever_type}\")\n",
    "    else:\n",
    "        raise Exception(f\"Unknown model type for model: {model_path}\")\n",
    "\n",
    "    retriever = EvaluateRetrieval(model, score_function=score_function, k_values=k_values)\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def reduce_precision(results, p=5):\n",
    "    return {k1:{k2:round(v2,p) for k2,v2 in v1.items()} for k1,v1 in results.items()}\n",
    "\n",
    "\n",
    "def eval(df, variables, retriever, metrics):\n",
    "    all_results = {}\n",
    "    all_scores = {}\n",
    "    all_qrels = {}\n",
    "\n",
    "    print(\"Iterating over groups...\")\n",
    "    for name, group in tqdm(df.groupby(\"research_data\")):\n",
    "        ivariables, mapping = get_instance_variables(group, variables)\n",
    "\n",
    "        if ivariables:\n",
    "\n",
    "            queries = get_queries(group)\n",
    "            corpus = get_corpus(group, ivariables, mapping)\n",
    "            qrels = get_qrels(group, mapping)\n",
    "            \n",
    "            data_dir = os.path.join(\".\", \"temp\", \"beir\", name)\n",
    "            save_files(queries, corpus, qrels, data_dir)\n",
    "            \n",
    "            corpus, queries, qrels = GenericDataLoader(data_folder=data_dir).load(split=\"all\")\n",
    "\n",
    "            results = retriever.retrieve(corpus, queries, return_sorted=True)\n",
    "            results = reduce_precision(results)\n",
    "            all_results[name] = results\n",
    "\n",
    "            score = _evaluate(qrels, results, metrics)\n",
    "            all_scores[name] = {\"score\": score, \"n\": group.shape[0]}\n",
    "\n",
    "    return all_results, all_scores\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    input_files, \n",
    "    variables_file, \n",
    "    model_path, \n",
    "    output_dir, \n",
    "    batch_size, \n",
    "    score_function, \n",
    "    metrics, \n",
    "    k_values,\n",
    "    model_save_path,\n",
    "    retriever_type,\n",
    "    show_progress_bar,\n",
    "    ):\n",
    "\n",
    "    assert os.path.exists(variables_file)\n",
    "    variables = load_variables(variables_file)\n",
    "\n",
    "    # Normalize metrics\n",
    "    metrics = get_metrics(metrics)\n",
    "\n",
    "    retriever = load_retriever(model_path, batch_size, score_function, k_values, retriever_type, show_progress_bar)\n",
    "\n",
    "    for input_file in input_files:\n",
    "        assert os.path.exists(input_file)\n",
    "        df = pd.read_csv(input_file, sep=\"\\t\")\n",
    "        df = df[df[\"research_data\"].notna()]  # drop rows w/o research data\n",
    "        df = drop_unk_variables(df)  # only evaluate on valid rows\n",
    "        langs = \"-\".join(list(set(df[\"lang\"].to_list())))\n",
    "        if os.path.exists(model_save_path):\n",
    "            output_subdir = os.path.join(output_dir, langs, os.path.basename(model_save_path))\n",
    "        else:\n",
    "            output_subdir = os.path.join(output_dir, langs, model_save_path)\n",
    "        if not os.path.exists(output_subdir):\n",
    "            os.makedirs(output_subdir)\n",
    "        print(f\"Input file contains {df.shape[0]} instances.\")\n",
    "\n",
    "        _type = \"\"\n",
    "        if \"explicit\" in input_file:\n",
    "            _type = \"explicit\"\n",
    "        elif \"other\" in input_file:\n",
    "            _type = \"other\"\n",
    "        elif \"train_unreleased\" in input_file:\n",
    "            _type = \"train_unreleased\"\n",
    "        elif \"test.tsv\" in input_file:\n",
    "            _type = \"full_test\"\n",
    "        elif \"train\" in input_file:\n",
    "            _type = \"train\"\n",
    "        elif \"test\" in input_file:\n",
    "            _type = \"test\"\n",
    "        else:\n",
    "            timestamp = datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y%m%d-%H%M%S\")\n",
    "            _type = timestamp\n",
    "\n",
    "        df = drop_unk_variables(df)\n",
    "        results, scores = eval(df, variables, retriever, metrics)\n",
    "\n",
    "        mean_scores = get_mean_scores(scores)\n",
    "        print(mean_scores)\n",
    "\n",
    "        if output_dir:\n",
    "                        \n",
    "            with open(os.path.join(output_subdir, f\"{_type}_results.json\"), \"w\") as fp:\n",
    "                json.dump(results, fp)\n",
    "\n",
    "            with open(os.path.join(output_subdir, f\"{_type}_scores.json\"), \"w\") as fp:\n",
    "                json.dump(mean_scores, fp)\n",
    "            \n",
    "            if _type == \"full_test\" or retriever_type.split(\"-\")[0] in [\"BM25\", \"BM25\", \"sparse\"]:\n",
    "                # Submission\n",
    "                submission = {k:v for rd in results for k,v in results[rd].items()}\n",
    "                with open(os.path.join(output_subdir, \"submission.json\"), \"w\") as fp:\n",
    "                    json.dump(submission, fp)\n",
    "\n",
    "            print(f\"Saved outputs to: {output_subdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode_path: choose from BM25, local paths, or huggingface hub models\n",
    "# retriever_type: choose from BM25-english, BM25-german, sparse, dense\n",
    "# If using BM25, make sure that Elasticsearch is running in the background.\n",
    "# The baseline in the paper used Elasticsearch 7.9.2 (https://www.elastic.co/downloads/past-releases/elasticsearch-7-9-2)\n",
    "\n",
    "model_path=\"BM25\"\n",
    "retriever_type=\"BM25-english\"\n",
    "dataset_names=None\n",
    "dataset_indicies=None\n",
    "num_epochs=1\n",
    "batch_size_pairs=64\n",
    "batch_size_triplets=32\n",
    "max_seq_length=128\n",
    "no_amp=False\n",
    "warmup_steps=500\n",
    "subset=None\n",
    "seeds=[0]\n",
    "eval_input_files=[\"./sv-ident/data/train/test.tsv\"]\n",
    "eval_variables_file=\"./sv-ident/data/train/variables_metadata.json\"\n",
    "eval_output_dir=\"./results/\"\n",
    "eval_batch_size=8\n",
    "eval_score_function=\"cos_sim\"\n",
    "eval_metrics=[\"map@10,30,50\", \"p@1,5,10,30\", \"r@1,5,10,30\", \"rprec\"]\n",
    "eval_k_values=[1,5,10,20]   \n",
    "show_progress_bar=False\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seeds(seed)  # for reproducibility\n",
    "    \n",
    "    if model_path != \"BM25\" and retriever_type == \"dense\":\n",
    "        word_embedding_model = models.Transformer(model_path, max_seq_length=max_seq_length)\n",
    "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "        model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "        model.show_progress_bar = False\n",
    "        \n",
    "        model_save_path = f\"seed={seed}_\"+os.path.basename(model_path).replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    else:\n",
    "        model = None\n",
    "        model_save_path = f\"seed={seed}_\"+os.path.basename(model_path).replace(\"/\", \"-\")+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate(\n",
    "        eval_input_files, \n",
    "        eval_variables_file, \n",
    "        model if model else model_path, \n",
    "        eval_output_dir, \n",
    "        eval_batch_size, \n",
    "        eval_score_function, \n",
    "        eval_metrics, \n",
    "        eval_k_values,\n",
    "        os.path.basename(model_save_path),\n",
    "        retriever_type,\n",
    "        show_progress_bar,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "45759ec2efc2df2d0a657003e975b93d270ed5dff98defa52318127f4d63bf08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
