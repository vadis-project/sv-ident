{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import KFold\n",
    "import subprocess\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../scripts\")\n",
    "from evaluate_task2 import DEFAULT_METRICS, evaluate"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tony/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# === Set your hyperparameters here ===\n",
    "\n",
    "# Choose the dataset to use from [\"en\", \"de\"]\n",
    "lang = \"de\"\n",
    "assert lang in [\"en\", \"de\"]\n",
    "\n",
    "# Directory where data is stored\n",
    "data_dir = \"../../data/\"\n",
    "assert os.path.exists(data_dir)\n",
    "\n",
    "# Cross-validation n splits\n",
    "n_splits = 10\n",
    "\n",
    "k_values = [1, 3, 5, 10]\n",
    "\n",
    "# ================ End ================"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load data\n",
    "\n",
    "# data_path = f'../../data/trial/train/{lang}.tsv'\n",
    "variables_path = f'../../data/trial/vocabulary/{lang}.tsv'\n",
    "\n",
    "# data_df = pd.read_csv(data_path, sep ='\\t')\n",
    "variable_df = pd.read_csv(variables_path, sep ='\\t')\n",
    "\n",
    "# data_df = data_df[data_df['is_variable']==1].reset_index(drop=True)\n",
    "data_df = pd.concat([pd.read_csv(os.path.join(data_dir, \"trial\", \"train\", lang+\".tsv\"),sep=\"\\t\"), pd.read_csv(os.path.join(data_dir, \"trial\", \"test\", lang+\".tsv\"),sep=\"\\t\")])\n",
    "data_df.rename(columns={\"is_variable\": \"label\"}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Text Preprocessor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def text_preprocess(ds: pd.Series) -> pd.Series:\n",
    "\n",
    "    for m in range(len(ds)):\n",
    "        \n",
    "        main_words = re.sub('[^a-zA-Z]', ' ', str(ds[m]))                                      # Retain only alphabets\n",
    "        main_words = (main_words.lower()).split()\n",
    "        main_words = [w for w in main_words if not w in set(stopwords.words('english'))]  # Remove stopwords\n",
    "        \n",
    "        lem = WordNetLemmatizer()\n",
    "        main_words = [lem.lemmatize(w) for w in main_words if len(w) > 1]                 # Group different forms of the same word\n",
    "        \n",
    "        main_words = ' '.join(main_words)\n",
    "        ds[m] = main_words\n",
    "\n",
    "    return ds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create Bag of Words Matrices\n",
    "\n",
    "BoW vocabulary is created using the variable detection dataset. This vocabulary is used to create matrices for both datasets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X_idx = data_df.index.to_numpy()\n",
    "vX = variable_df.question.to_numpy()\n",
    "\n",
    "kf = KFold(n_splits=n_splits)\n",
    "kf.get_n_splits(X_idx)\n",
    "\n",
    "all_res = defaultdict(list)\n",
    "\n",
    "# Train model with cross-validation\n",
    "print(\"Training models with cross-validation...\")\n",
    "for i, (train_index, test_index) in enumerate(tqdm(kf.split(X_idx), total=n_splits)):\n",
    "    train_dataset = datasets.Dataset.from_pandas(data_df.iloc[train_index])\n",
    "    test_dataset = datasets.Dataset.from_pandas(data_df.iloc[test_index])\n",
    "\n",
    "    X = text_preprocess(train_dataset['text'])\n",
    "    vX = text_preprocess(variable_df['question'])\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(X).toarray()\n",
    "    vX = vectorizer.transform(vX).toarray()\n",
    "\n",
    "    # ===\n",
    "    results = {}\n",
    "\n",
    "    # Create a list for indices of variable that are mentioned in the dataset.\n",
    "    variable_idx_list = []\n",
    "    variable_id_list = variable_df['id'].to_list()\n",
    "    for i,row in data_df.iterrows():\n",
    "        variables = row['variable'].split(',')\n",
    "        pos_variables = ['v'+var.split('-')[0] for var in variables if 'yes' in var.lower()]\n",
    "        pos_variable_idx = [variable_id_list.index(var) for var in pos_variables if var in variable_id_list]\n",
    "        variable_idx_list.append(pos_variable_idx)\n",
    "\n",
    "    tX = vectorizer.transform(text_preprocess(test_dataset['text'])).toarray()\n",
    "    assert len(tX) == len(test_dataset)\n",
    "    assert len(vX) == len(variable_df)\n",
    "    for x,td in zip(tX,test_dataset[\"uuid\"]):\n",
    "        scores = {}\n",
    "\n",
    "        for v,vd in zip(vX,variable_df[\"id\"]):\n",
    "            score = cosine_similarity([x], [v])[0][0]\n",
    "            scores[vd] = score\n",
    "        \n",
    "        results[td] = scores\n",
    "\n",
    "    # ===\n",
    "    qrels = {}\n",
    "\n",
    "    for row in test_dataset:\n",
    "        _id = row[\"uuid\"]\n",
    "        _variables = {}\n",
    "\n",
    "        if row[\"variable\"] in [\"No\", \"NoSkip\"]:\n",
    "            # qrels[_id] = _variables\n",
    "            continue\n",
    "\n",
    "        for v in row[\"variable\"].split(\",\"):\n",
    "            v_id = \"v\"+v.split(\"-\")[0]\n",
    "            if \"yes\" in v.lower():\n",
    "                _variables[v_id] = 1\n",
    "            elif \"no\" in v.lower():\n",
    "                _variables[v_id] = 0\n",
    "        \n",
    "        if _variables != {}:\n",
    "            qrels[_id] = _variables\n",
    "    \n",
    "    # Save outputs\n",
    "\n",
    "    with open(\"./run_bow.json\", \"w\") as fp:\n",
    "        json.dump(results, fp)\n",
    "\n",
    "    with open(\"./qrels_bow.json\", \"w\") as fp:\n",
    "        json.dump(qrels, fp)\n",
    "    \n",
    "    # Evaluate\n",
    "    res = evaluate(qrels_path=\"./qrels_bow.json\", run_path=\"./run_bow.json\", metrics_str=DEFAULT_METRICS)\n",
    "    for k,v in res.items():\n",
    "        all_res[k].append(v)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training models with cross-validation...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08315116fa844e5989967362ab3844ca"
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Rprec': 0.07692, 'P@1': 0.07692, 'P@3': 0.02564, 'P@5': 0.01538, 'P@10': 0.02308, 'P@20': 0.01538, 'recall@1': 0.07692, 'recall@3': 0.07692, 'recall@5': 0.07692, 'recall@10': 0.15385, 'recall@20': 0.19231, 'map@cut@20': 0.09615, 'map@cut@50': 0.10367}\n",
      "{'Rprec': 0.10526, 'P@1': 0.10526, 'P@3': 0.05263, 'P@5': 0.03158, 'P@10': 0.02105, 'P@20': 0.01316, 'recall@1': 0.10526, 'recall@3': 0.15789, 'recall@5': 0.15789, 'recall@10': 0.21053, 'recall@20': 0.26316, 'map@cut@20': 0.13487, 'map@cut@50': 0.14174}\n",
      "{'Rprec': 0.06667, 'P@1': 0.06667, 'P@3': 0.04444, 'P@5': 0.04, 'P@10': 0.02667, 'P@20': 0.02, 'recall@1': 0.06667, 'recall@3': 0.13333, 'recall@5': 0.2, 'recall@10': 0.23333, 'recall@20': 0.31667, 'map@cut@20': 0.11475, 'map@cut@50': 0.11765}\n",
      "{'Rprec': 0.11538, 'P@1': 0.15385, 'P@3': 0.05128, 'P@5': 0.04615, 'P@10': 0.03846, 'P@20': 0.03462, 'recall@1': 0.11538, 'recall@3': 0.11538, 'recall@5': 0.19231, 'recall@10': 0.30769, 'recall@20': 0.5, 'map@cut@20': 0.16615, 'map@cut@50': 0.16945}\n",
      "{'Rprec': 0.13333, 'P@1': 0.2, 'P@3': 0.08889, 'P@5': 0.06667, 'P@10': 0.04, 'P@20': 0.03, 'recall@1': 0.13333, 'recall@3': 0.2, 'recall@5': 0.26667, 'recall@10': 0.33333, 'recall@20': 0.5, 'map@cut@20': 0.20413, 'map@cut@50': 0.21058}\n",
      "{'Rprec': 0.19444, 'P@1': 0.22222, 'P@3': 0.09259, 'P@5': 0.05556, 'P@10': 0.02778, 'P@20': 0.01944, 'recall@1': 0.19444, 'recall@3': 0.25, 'recall@5': 0.25, 'recall@10': 0.25, 'recall@20': 0.33333, 'map@cut@20': 0.21798, 'map@cut@50': 0.22063}\n",
      "{'Rprec': 0.07692, 'P@1': 0.07692, 'P@3': 0.02564, 'P@5': 0.01538, 'P@10': 0.03846, 'P@20': 0.02308, 'recall@1': 0.07692, 'recall@3': 0.07692, 'recall@5': 0.07692, 'recall@10': 0.25, 'recall@20': 0.26923, 'map@cut@20': 0.1042, 'map@cut@50': 0.11085}\n",
      "{'Rprec': 0.0, 'P@1': 0.0, 'P@3': 0.02222, 'P@5': 0.01333, 'P@10': 0.00667, 'P@20': 0.00333, 'recall@1': 0.0, 'recall@3': 0.06667, 'recall@5': 0.06667, 'recall@10': 0.06667, 'recall@20': 0.06667, 'map@cut@20': 0.03333, 'map@cut@50': 0.04482}\n",
      "{'Rprec': 0.08333, 'P@1': 0.06667, 'P@3': 0.06667, 'P@5': 0.06667, 'P@10': 0.04667, 'P@20': 0.03, 'recall@1': 0.06667, 'recall@3': 0.2, 'recall@5': 0.23333, 'recall@10': 0.31667, 'recall@20': 0.45, 'map@cut@20': 0.16763, 'map@cut@50': 0.17021}\n",
      "{'Rprec': 0.14286, 'P@1': 0.14286, 'P@3': 0.07143, 'P@5': 0.04286, 'P@10': 0.02857, 'P@20': 0.01429, 'recall@1': 0.14286, 'recall@3': 0.21429, 'recall@5': 0.21429, 'recall@10': 0.25, 'recall@20': 0.25, 'map@cut@20': 0.17113, 'map@cut@50': 0.18285}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import statistics\n",
    "# Compute mean and standard deviation\n",
    "print(\"***** Cross-Validation Results *****\")\n",
    "for k, v in all_res.items():\n",
    "    mean, std, pstd = (\n",
    "        statistics.mean(v),\n",
    "        statistics.stdev(v),\n",
    "        statistics.pstdev(v),\n",
    "    )\n",
    "    print(\n",
    "        k + \":\\n\",\n",
    "        \"Mean:\",\n",
    "        round(mean, 4),\n",
    "        \"\\tStd.:\",\n",
    "        round(std, 4),\n",
    "        \"\\tPStd:\",\n",
    "        round(pstd, 4),\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "***** Cross-Validation Results *****\n",
      "Rprec:\n",
      " Mean: 0.0995 \tStd.: 0.0523 \tPStd: 0.0496\n",
      "P_1:\n",
      " Mean: 0.1111 \tStd.: 0.0681 \tPStd: 0.0646\n",
      "P_3:\n",
      " Mean: 0.0541 \tStd.: 0.0256 \tPStd: 0.0243\n",
      "P_5:\n",
      " Mean: 0.0394 \tStd.: 0.0203 \tPStd: 0.0192\n",
      "P_10:\n",
      " Mean: 0.0297 \tStd.: 0.0116 \tPStd: 0.011\n",
      "P_20:\n",
      " Mean: 0.0203 \tStd.: 0.0094 \tPStd: 0.009\n",
      "recall_1:\n",
      " Mean: 0.0978 \tStd.: 0.0531 \tPStd: 0.0504\n",
      "recall_3:\n",
      " Mean: 0.1491 \tStd.: 0.0652 \tPStd: 0.0619\n",
      "recall_5:\n",
      " Mean: 0.1735 \tStd.: 0.0754 \tPStd: 0.0715\n",
      "recall_10:\n",
      " Mean: 0.2372 \tStd.: 0.0801 \tPStd: 0.076\n",
      "recall_20:\n",
      " Mean: 0.3141 \tStd.: 0.1385 \tPStd: 0.1314\n",
      "map_cut_20:\n",
      " Mean: 0.141 \tStd.: 0.0556 \tPStd: 0.0528\n",
      "map_cut_50:\n",
      " Mean: 0.1472 \tStd.: 0.0541 \tPStd: 0.0513\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9c75a10141fb9765b6dd95572dd5e09c2b8655ce454a09c7fa750dc40f7865b"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}