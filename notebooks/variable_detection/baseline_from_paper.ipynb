{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import statistics\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset, lowercase=False, batch_size=16, max_len=64, instance_col_name=\"text\"):\n",
    "        self.dataset = dataset\n",
    "        self.lowercase = lowercase\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.instance_col_name = instance_col_name\n",
    "\n",
    "    def _encode(self, example):\n",
    "        return self.tokenizer(\n",
    "            example[self.instance_col_name],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    def format(self, dataset):\n",
    "        dataset = dataset.map(self._encode, batched=True)\n",
    "        try:\n",
    "            dataset.set_format(\n",
    "                type=\"torch\",\n",
    "                columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"],\n",
    "            )\n",
    "        except:\n",
    "            try:\n",
    "                dataset.set_format(\n",
    "                type=\"torch\",\n",
    "                columns=[\"input_ids\", \"attention_mask\", \"label\"],\n",
    "            )\n",
    "            except:\n",
    "                raise Exception(\"Unable to set columns.\")\n",
    "        return dataset\n",
    "\n",
    "    def format_data(self, tokenizer, batch_size=None):\n",
    "        print(\"Formatting data...\")\n",
    "        self.tokenizer = tokenizer\n",
    "        if batch_size:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        self.train_dataset = self.format(self.dataset[\"train\"])\n",
    "        # self.validation_dataset = self.format(self.dataset[\"validation\"])\n",
    "        self.test_dataset = self.format(self.dataset[\"test\"])\n",
    "        print(\"Done formatting.\")\n",
    "\n",
    "\n",
    "def make_output_dir(output_dir):\n",
    "    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = os.path.join(output_dir, timestamp)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "def set_seed(seed):  # for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def compute_metrics(p, average=\"macro\"):\n",
    "    pred, true = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=true, y_pred=pred)\n",
    "    recall = recall_score(y_true=true, y_pred=pred, average=average)\n",
    "    precision = precision_score(y_true=true, y_pred=pred, average=average)\n",
    "    f1 = f1_score(y_true=true, y_pred=pred, average=average)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1-score\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=None,\n",
    "        lowercase=False,\n",
    "        max_len=64,\n",
    "        num_labels=2,\n",
    "        output_dir=\"\",\n",
    "        seed=None,  # for reproducibility\n",
    "    ):\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Model parameters\n",
    "        self.model_name = model_name\n",
    "        self.do_lower_case = lowercase\n",
    "        self.max_len = max_len\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Set seed for reproducibility\n",
    "        if seed:  # for reproducibility\n",
    "            set_seed(seed)\n",
    "\n",
    "        print(\"Loading pre-trained tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)\n",
    "\n",
    "        try:\n",
    "            self.tokenizer.do_lower_case = self.do_lower_case\n",
    "        except:\n",
    "            try:\n",
    "                self.tokenizer.do_lowercase_and_remove_accent = self.do_lower_case\n",
    "            except:\n",
    "                raise Exception(\"Unable to set value for 'do_lower_case' or 'do_lowercase_and_remove_accent'\")\n",
    "        self.tokenizer.model_max_length = self.max_len\n",
    "        print(\"Done.\")\n",
    "\n",
    "    def model_init(self):\n",
    "        print(\"Loading pre-trained model...\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, \n",
    "            num_labels = self.num_labels,\n",
    "        )\n",
    "        print(f\"Loaded model from path: {self.model_name}\")\n",
    "        return self.model\n",
    "\n",
    "    def save_model(self, save_path=None):\n",
    "        save_dir = save_path if save_path else self.output_dir\n",
    "        if not os.path.isdir(save_dir):\n",
    "            save_dir.mkdir(parents=True)\n",
    "        torch.save(self.model, save_dir)\n",
    "        print(f\"Saved model to path: {save_dir}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        if os.path.exists(path):\n",
    "            print(\"Loading local model state dict...\")\n",
    "            self.model.load_state_dict(torch.load(path))\n",
    "            print(f\"Loaded model from path: {path}\")\n",
    "        else:\n",
    "            print(f\"Model path does not exist: {path}\")\n",
    "            raise Exception(f\"The specified file path ({path}) does not exist!\")\n",
    "\n",
    "\n",
    "class TextClassifierTrainer(TextClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir=\"\",\n",
    "        # bfloat16=False,\n",
    "        epochs=5,\n",
    "        batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.9,\n",
    "        random_weights=False,\n",
    "        load_best_model_at_end=False,\n",
    "        early_stop=False,\n",
    "        eval_steps=500,\n",
    "        seed=None,  # for reproducibility\n",
    "        save_strategy=\"no\",\n",
    "        no_cuda=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dir = output_dir\n",
    "        self.logging_dir = os.path.join(output_dir, \"logs\")\n",
    "\n",
    "        # Model parameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.load_best_model_at_end = load_best_model_at_end if save_strategy != \"no\" else False\n",
    "        self.random_weights = random_weights\n",
    "        self.early_stop = early_stop\n",
    "        self.eval_steps = eval_steps\n",
    "        self.save_strategy = save_strategy\n",
    "        self.no_cuda = no_cuda\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "    def train(self, train_dataset, eval_dataset):\n",
    "\n",
    "        ### Training\n",
    "        print(\"Initializing trainer...\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=self.epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size * 4,\n",
    "            warmup_steps=self.warmup_steps,\n",
    "            weight_decay=self.weight_decay,\n",
    "            # logging_dir=self.logging_dir,\n",
    "            # logging_strategy=\"steps\",\n",
    "            # logging_steps=self.eval_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=self.eval_steps,\n",
    "            load_best_model_at_end=self.load_best_model_at_end,\n",
    "            save_strategy=self.save_strategy,\n",
    "            # report_to=\"wandb\",\n",
    "            seed=self.seed,\n",
    "            no_cuda=self.no_cuda,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model_init=self.model_init,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "        print(\"Trainer initialized.\")\n",
    "        print(\"Training...\")\n",
    "        trainer.train()\n",
    "        print(\"Done training.\")\n",
    "\n",
    "        ### Validation\n",
    "        print(\"Evaluating...\")\n",
    "        res = trainer.evaluate()\n",
    "        preds = trainer.predict(eval_dataset)\n",
    "        raw_preds = preds[0]\n",
    "        labels = [np.argmax(l) for l in raw_preds]\n",
    "\n",
    "        raw_res = {}\n",
    "        assert len(labels) == len(eval_dataset)\n",
    "        for uuid,label in zip(eval_dataset[\"uuid\"],labels):\n",
    "            raw_res[uuid] = str(label)\n",
    "        print(\"Evaluation results:\")\n",
    "        print(\n",
    "            f'Eval loss: \\t{res[\"eval_loss\"]}, Eval Acc: \\t{res[\"eval_accuracy\"]}, Eval P: \\t{res[\"eval_precision\"]}, Eval R: \\t{res[\"eval_recall\"]}, Eval F1: \\t{res[\"eval_f1-score\"]}'\n",
    "        )\n",
    "        print(\"Done evaluating.\")\n",
    "\n",
    "        if self.save_strategy != \"no\":\n",
    "            trainer.save_model()\n",
    "            trainer.save_state()\n",
    "            self.trained_model_path = os.path.join(self.output_dir, \"pytorch_model.bin\")\n",
    "            assert self.trained_model_path.is_file()\n",
    "            print(f\"Saved model to path: {self.output_dir}\")\n",
    "\n",
    "        return res, raw_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, test_dataset, args):\n",
    "    run_output_dir = args.experiment_output_dir\n",
    "    if args.save_strategy != \"no\":\n",
    "        run_output_dir = make_output_dir(args.experiment_output_dir)\n",
    "\n",
    "    trainer = TextClassifierTrainer(\n",
    "        model_name=args.model,\n",
    "        lowercase=args.lowercase,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        output_dir=run_output_dir,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        weight_decay=args.weight_decay,\n",
    "        load_best_model_at_end=args.load_best_model_at_end,\n",
    "        random_weights=args.random_weights,\n",
    "        early_stop=args.early_stop,\n",
    "        eval_steps=args.eval_steps,\n",
    "        save_strategy=args.save_strategy,\n",
    "        seed=args.seed,\n",
    "        no_cuda=args.no_cuda,\n",
    "    )\n",
    "\n",
    "    dataset = Dataset({\"train\": train_dataset, \"test\": test_dataset}, instance_col_name=args.instance_col_name)\n",
    "    dataset.format_data(trainer.tokenizer)\n",
    "\n",
    "    result, raw_result = trainer.train(dataset.train_dataset, dataset.test_dataset)\n",
    "\n",
    "    return result, raw_result\n",
    "\n",
    "\n",
    "def load_trainer(args):\n",
    "    run_output_dir = args.experiment_output_dir\n",
    "    if args.save_strategy != \"no\":\n",
    "        run_output_dir = make_output_dir(args.experiment_output_dir)\n",
    "\n",
    "    trainer = TextClassifierTrainer(\n",
    "        model_name=args.model,\n",
    "        lowercase=args.lowercase,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "        output_dir=run_output_dir,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        weight_decay=args.weight_decay,\n",
    "        load_best_model_at_end=args.load_best_model_at_end,\n",
    "        random_weights=args.random_weights,\n",
    "        early_stop=args.early_stop,\n",
    "        eval_steps=args.eval_steps,\n",
    "        save_strategy=args.save_strategy,\n",
    "        seed=args.seed,\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def run(args):\n",
    "    # wandb.init(project=args.wandb_name)\n",
    "\n",
    "    if args.train_data:\n",
    "        if isinstance(args.train_data, list):\n",
    "            try:\n",
    "                for path in args.train_data:\n",
    "                    assert os.path.exists(path)\n",
    "            except:\n",
    "                raise Exception(f\"Failed to load: {args.train_data}\")\n",
    "\n",
    "            train_dfs = []\n",
    "            for path in args.train_data:\n",
    "                _df = pd.read_csv(path, sep=\"\\t\")\n",
    "                if args.lang:\n",
    "                    _df = _df[_df[\"lang\"] == args.lang]\n",
    "                train_dfs.append(_df)\n",
    "\n",
    "            train_df = pd.concat(train_dfs)\n",
    "        elif isinstance(args.train_data, pd.DataFrame):\n",
    "            train_df = args.train_data\n",
    "        else:\n",
    "            raise Exception(f\"Invalid type for {type(args.train_data)}\")\n",
    "\n",
    "        train_df.rename(columns={\"is_variable\": \"label\"}, inplace=True)\n",
    "    else:\n",
    "        train_df = None\n",
    "    \n",
    "    if args.test_data:\n",
    "        if isinstance(args.test_data, list):\n",
    "            try:\n",
    "                for path in args.test_data:\n",
    "                    assert os.path.exists(path)\n",
    "            except:\n",
    "                raise Exception(f\"Failed to load: {args.test_data}\")\n",
    "\n",
    "            test_dfs = []\n",
    "            for path in args.test_data:\n",
    "                _df = pd.read_csv(path, sep=\"\\t\")\n",
    "                test_dfs.append(_df)\n",
    "\n",
    "            test_df = pd.concat(test_dfs)\n",
    "        elif isinstance(args.test_data, pd.DataFrame):\n",
    "            test_df = args.test_data\n",
    "        else:\n",
    "            raise Exception(f\"Invalid type for {type(args.test_data)}\")\n",
    "        \n",
    "        test_df.rename(columns={\"is_variable\": \"label\"}, inplace=True)\n",
    "    else:\n",
    "        test_df = None\n",
    "\n",
    "    if isinstance(train_df, pd.DataFrame):\n",
    "        assert args.instance_col_name in train_df.columns.tolist()\n",
    "    if isinstance(test_df, pd.DataFrame):\n",
    "        assert args.instance_col_name in test_df.columns.tolist()\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    raw_results = {}\n",
    "\n",
    "    experiment_output_dir = make_output_dir(args.output_dir)\n",
    "    args.experiment_output_dir = experiment_output_dir\n",
    "\n",
    "    test_indices = []\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "\n",
    "    if args.train:\n",
    "        # Train model\n",
    "        print(f\"Training model...\")\n",
    "\n",
    "        if args.do_cross_validation:\n",
    "            print(\"Running cross-validation...\")\n",
    "\n",
    "            combined_df = pd.concat([train_df, test_df]).copy()\n",
    "            # combined_df.reset_index(inplace=True)\n",
    "            X_idx = combined_df.index.to_numpy()\n",
    "            y = combined_df.label.to_numpy()\n",
    "\n",
    "            if args.balance_splits:\n",
    "                print(\"Balacing splits...\")\n",
    "                kf = StratifiedKFold(n_splits=args.n_cv_splits, random_state=args.seed, shuffle=True)\n",
    "                kf.get_n_splits(X_idx)\n",
    "                splits = kf.split(X_idx, y)\n",
    "            else:\n",
    "                kf = KFold(n_splits=args.n_cv_splits, random_state=args.seed, shuffle=True)\n",
    "                kf.get_n_splits(X_idx)\n",
    "                splits = kf.split(X_idx)\n",
    "\n",
    "            for j, (train_index, test_index) in enumerate(tqdm(splits)):\n",
    "                train_dataset = datasets.Dataset.from_pandas(combined_df.iloc[train_index])\n",
    "                test_dataset = datasets.Dataset.from_pandas(combined_df.iloc[test_index])\n",
    "                test_indices.append(test_index)\n",
    "\n",
    "                result, raw_result = train(train_dataset, test_dataset, args)\n",
    "\n",
    "                for k, v in result.items():\n",
    "                    results[k].append(v)\n",
    "                    scores[k].append(v)\n",
    "                \n",
    "                raw_results[str(j)] = raw_result\n",
    "        else:\n",
    "            train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "            test_dataset = datasets.Dataset.from_pandas(test_df)\n",
    "\n",
    "            result, raw_result = train(train_dataset, test_dataset, args)\n",
    "\n",
    "            for k, v in result.items():\n",
    "                results[k].append(v)\n",
    "                scores[k].append(v)\n",
    "            \n",
    "            raw_results[\"0\"] = raw_result\n",
    "\n",
    "        # Save hyperparameters\n",
    "        results_file = os.path.join(experiment_output_dir, \"hyperparameters.jsonl\")\n",
    "        with jsonlines.open(results_file, \"a\") as writer:\n",
    "            writer.write(vars(args))\n",
    "\n",
    "        # Compute mean and standard deviation\n",
    "        print(\"***** Cross-Validation Results *****\")\n",
    "        for k, v in results.items():\n",
    "            skip = True\n",
    "            for m in [\"accuracy\", \"precision\", \"recall\", \"f1-score\"]:\n",
    "                if m in k:\n",
    "                    skip = False\n",
    "            if skip:\n",
    "                continue\n",
    "            mean, std, pstd = (\n",
    "                statistics.mean(v) if len(v) > 1 else v[0],\n",
    "                statistics.stdev(v) if len(v) > 1 else 0,\n",
    "                statistics.pstdev(v) if len(v) > 1 else 0,\n",
    "            )\n",
    "            print(\n",
    "                k + \":\\n\",\n",
    "                \"Mean:\",\n",
    "                round(mean, 4),\n",
    "                \"\\tStd.:\",\n",
    "                round(std, 4),\n",
    "                \"\\tPStd:\",\n",
    "                round(pstd, 4),\n",
    "            )\n",
    "            \n",
    "            with jsonlines.open(results_file, \"a\") as writer:\n",
    "                writer.write({k: {\"Mean\": mean, \"Std\": std, \"PStd\": pstd}})\n",
    "        \n",
    "        with open(os.path.join(experiment_output_dir, \"results_raw.jsonl\"), \"w\") as fp:\n",
    "            json.dump(scores, fp)\n",
    "        \n",
    "        # Save predictions\n",
    "        raw_results_file = os.path.join(experiment_output_dir, \"predictions.json\")\n",
    "        with open(raw_results_file, \"w\") as fp:\n",
    "            json.dump(raw_results, fp)\n",
    "\n",
    "        # Save the prediction as a submission file\n",
    "        raw_results_file = os.path.join(experiment_output_dir, \"submission.json\")\n",
    "        with open(raw_results_file, \"w\") as fp:\n",
    "            json.dump(raw_results['0'], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    train=True\n",
    "    test=False\n",
    "    lang=None\n",
    "    train_data=[\"./sv-ident/data/train/train.tsv\", \"./sv-ident/data/train/val.tsv\"]\n",
    "    test_data=[]\n",
    "    instance_col_name=\"sentence\"\n",
    "    # wandb-name=\"variable-detection-baselines-FT\"\n",
    "    model=\"KM4STfulltext/SSCI-SciBERT-e2\"\n",
    "    output_dir=\"./runs/sv-ident/cross-val/en_sub/\"\n",
    "    max_len=64\n",
    "    lowercase=False\n",
    "    gradient_accumulation_steps=1\n",
    "    learning_rate=5e-5\n",
    "    weight_decay=0.0\n",
    "    adam_epsilon=1e-8\n",
    "    max_grad_norm=1.0\n",
    "    random_weights=False\n",
    "    batch_size=16\n",
    "    epochs=1\n",
    "    max_steps=-1\n",
    "    warmup_steps=0\n",
    "    early_stop=False\n",
    "    eval_steps=100\n",
    "    num_labels=2\n",
    "    save_strategy=\"no\"\n",
    "    load_best_model_at_end=False\n",
    "    seed=0\n",
    "    balance_splits=False\n",
    "    do_cross_validation=True\n",
    "    n_cv_splits=10\n",
    "    no_cuda=False\n",
    "\n",
    "args = Args()\n",
    "assert (args.train or args.test)\n",
    "run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "523cda30a0dadc85c735072c12efa25dcb32a644a23d56c4367852b3f0279ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
