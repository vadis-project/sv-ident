{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import datetime\n",
    "import os\n",
    "import statistics\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "import datasets\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# === Set your hyperparameters here ===\n",
    "\n",
    "# Choose the dataset to use from [\"en\", \"de\"]\n",
    "lang = \"en\"\n",
    "assert lang in [\"en\", \"de\"]\n",
    "\n",
    "# Any HuggingFace model (https://huggingface.co/models) works \n",
    "model = \"bert-base-cased\"\n",
    "\n",
    "# Directory where to save different training runs\n",
    "output_dir = \"./logs/runs/\"\n",
    "\n",
    "# Directory where data is stored\n",
    "data_dir = \"../../data/\"\n",
    "assert os.path.exists(data_dir)\n",
    "\n",
    "# Cut or pad sequences to this length\n",
    "max_len = 64\n",
    "assert isinstance(max_len, int)\n",
    "\n",
    "# Lowercase text (this should match the model used; i.e., when using bert-base-cased set this to True)\n",
    "lowercase = False\n",
    "assert isinstance(lowercase, bool)\n",
    "\n",
    "# Other training-specific hyperparameters\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.0\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1.0\n",
    "random_weights = False\n",
    "batch_size = 5\n",
    "epochs = 5\n",
    "max_steps = -1\n",
    "warmup_steps = 0\n",
    "early_stop = False\n",
    "eval_steps = 100\n",
    "num_labels = 2\n",
    "# Strategy for saving model. Possible values: [\"no\", \"epoch\", \"steps\"]. See https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments.save_strategy\n",
    "save_strategy = \"no\"\n",
    "load_best_model_at_end = True  # This can only be true if save strategy is not \"no\"\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# ================ End ================"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_output_dir(output_dir):\n",
    "    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_dir = os.path.join(output_dir, timestamp)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    return output_dir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset, lowercase=False, batch_size=16, max_len=64):\n",
    "        self.dataset = dataset\n",
    "        self.lowercase = lowercase\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _encode(self, example):\n",
    "        return self.tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    def format(self, dataset):\n",
    "        dataset = dataset.map(self._encode, batched=True)\n",
    "        dataset.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"],\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "    def format_data(self, tokenizer, batch_size=None):\n",
    "        print(\"Formatting data...\")\n",
    "        self.tokenizer = tokenizer\n",
    "        if batch_size:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        self.train_dataset = self.format(self.dataset[\"train\"])\n",
    "        # self.validation_dataset = self.format(self.dataset[\"validation\"])\n",
    "        self.test_dataset = self.format(self.dataset[\"test\"])\n",
    "        print(\"Done formatting.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def compute_metrics(p, average=\"macro\"):\n",
    "    pred, true = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=true, y_pred=pred)\n",
    "    recall = recall_score(y_true=true, y_pred=pred, average=average)\n",
    "    precision = precision_score(y_true=true, y_pred=pred, average=average)\n",
    "    f1 = f1_score(y_true=true, y_pred=pred, average=average)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1-score\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        lowercase=False,\n",
    "        max_len=64,\n",
    "        num_labels=2,\n",
    "        output_dir=\"\",\n",
    "        seed=None,\n",
    "    ):\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Model parameters\n",
    "        self.model_name = model_name\n",
    "        self.do_lower_case = lowercase\n",
    "        self.max_len = max_len\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Set seed for reproducibility\n",
    "        if seed:\n",
    "            set_seed(seed)\n",
    "\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        print(\"Loading pre-trained tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)\n",
    "\n",
    "        self.tokenizer.do_lower_case = self.do_lower_case\n",
    "        self.tokenizer.model_max_length = self.max_len\n",
    "        print(\"Done.\")\n",
    "\n",
    "        self.model_config = AutoConfig.from_pretrained(self.model_name)\n",
    "        self.model_config.num_labels = self.num_labels\n",
    "\n",
    "        print(\"Loading pre-trained model...\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, config=self.model_config\n",
    "        )\n",
    "        print(f\"Loaded model from path: {self.model_name}\")\n",
    "\n",
    "    def save_model(self, save_path=None):\n",
    "        save_dir = save_path if save_path else self.output_dir\n",
    "        if not os.path.isdir(save_dir):\n",
    "            save_dir.mkdir(parents=True)\n",
    "        torch.save(self.model, save_dir)\n",
    "        print(f\"Saved model to path: {save_dir}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        if os.path.exists(path):\n",
    "            print(\"Loading local model state dict...\")\n",
    "            self.model.load_state_dict(torch.load(path))\n",
    "            print(f\"Loaded model from path: {path}\")\n",
    "        else:\n",
    "            print(f\"Model path does not exist: {path}\")\n",
    "            raise Exception(f\"The specified file path ({path}) does not exist!\")\n",
    "\n",
    "\n",
    "class TextClassifierTrainer(TextClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir=\"\",\n",
    "        # bfloat16=False,\n",
    "        epochs=5,\n",
    "        batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.9,\n",
    "        random_weights=False,\n",
    "        load_best_model_at_end=False,\n",
    "        early_stop=False,\n",
    "        eval_steps=500,\n",
    "        seed=42,\n",
    "        save_strategy=\"no\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dir = output_dir\n",
    "        self.logging_dir = os.path.join(output_dir, \"logs\")\n",
    "\n",
    "        # Model parameters\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.load_best_model_at_end = load_best_model_at_end if save_strategy != \"no\" else False\n",
    "        self.random_weights = random_weights\n",
    "        self.early_stop = early_stop\n",
    "        self.eval_steps = eval_steps\n",
    "        self.save_strategy = save_strategy\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "    def train(self, train_dataset, eval_dataset):\n",
    "\n",
    "        ### Training\n",
    "        print(\"Initializing trainer...\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=self.epochs,\n",
    "            per_device_train_batch_size=self.batch_size,\n",
    "            per_device_eval_batch_size=self.batch_size * 4,\n",
    "            warmup_steps=self.warmup_steps,\n",
    "            weight_decay=self.weight_decay,\n",
    "            logging_dir=self.logging_dir,\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=self.eval_steps,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=self.eval_steps,\n",
    "            load_best_model_at_end=self.load_best_model_at_end,\n",
    "            save_strategy=self.save_strategy,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        print(\"Trainer initialized.\")\n",
    "        print(\"Training...\")\n",
    "        trainer.train()\n",
    "        print(\"Done training.\")\n",
    "\n",
    "        ### Validation\n",
    "        print(\"Evaluating...\")\n",
    "        res = trainer.evaluate()\n",
    "        print(\"Evaluation results:\")\n",
    "        print(\n",
    "            f'Eval loss: \\t{res[\"eval_loss\"]}, Eval Acc: \\t{res[\"eval_accuracy\"]}, Eval P: \\t{res[\"eval_precision\"]}, Eval R: \\t{res[\"eval_recall\"]}, Eval F1: \\t{res[\"eval_f1-score\"]}'\n",
    "        )\n",
    "        print(\"Done evaluating.\")\n",
    "\n",
    "        if self.save_strategy != \"no\":\n",
    "            trainer.save_model()\n",
    "            trainer.save_state()\n",
    "            self.trained_model_path = os.path.join(self.output_dir, \"pytorch_model.bin\")\n",
    "            assert self.trained_model_path.is_file()\n",
    "            print(f\"Saved model to path: {self.output_dir}\")\n",
    "\n",
    "        return res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(args):\n",
    "    # _filename = \"variable_detection_german_dev.tsv\" if args.data_lang == \"de\" else \"variable_detection_dev.tsv\"\n",
    "    # data_path = os.path.join(args.data_dir, _filename)\n",
    "    try:\n",
    "        assert os.path.exists(args.train_path)\n",
    "    except:\n",
    "        raise Exception(f\"Failed to load: {args.train_path}\")\n",
    "\n",
    "    train_df = pd.read_csv(args.train_path, sep=\"\\t\")\n",
    "    train_df.rename(columns={\"is_variable\": \"label\"}, inplace=True)\n",
    "\n",
    "    X_idx = train_df.index.to_numpy()\n",
    "    y = train_df.label.to_numpy()\n",
    "\n",
    "    kf = KFold(n_splits=10)\n",
    "    kf.get_n_splits(X_idx)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Train model with cross-validation\n",
    "    print(\"Training models with cross-validation...\")\n",
    "    for i, (train_index, test_index) in enumerate(tqdm(kf.split(X_idx))):\n",
    "        train_dataset = datasets.Dataset.from_pandas(train_df.iloc[train_index])\n",
    "        test_dataset = datasets.Dataset.from_pandas(train_df.iloc[test_index])\n",
    "\n",
    "        run_output_dir = args.output_dir\n",
    "        if args.save_strategy != \"no\":\n",
    "            run_output_dir = make_output_dir(args.output_dir)\n",
    "\n",
    "        trainer = TextClassifierTrainer(\n",
    "            model_name=args.model,\n",
    "            lowercase=args.lowercase,\n",
    "            epochs=args.epochs,\n",
    "            batch_size=args.batch_size,\n",
    "            output_dir=run_output_dir,\n",
    "            warmup_steps=args.warmup_steps,\n",
    "            weight_decay=args.weight_decay,\n",
    "            load_best_model_at_end=args.load_best_model_at_end,\n",
    "            random_weights=args.random_weights,\n",
    "            early_stop=args.early_stop,\n",
    "            eval_steps=args.eval_steps,\n",
    "            save_strategy=args.save_strategy,\n",
    "            seed=args.seed,\n",
    "        )\n",
    "\n",
    "        dataset = Dataset({\"train\": train_dataset, \"test\": test_dataset})\n",
    "        dataset.format_data(trainer.tokenizer)\n",
    "\n",
    "        result = trainer.train(dataset.train_dataset, dataset.test_dataset)\n",
    "\n",
    "        for k, v in result.items():\n",
    "            if k in results:\n",
    "                results[k] += [v]\n",
    "            else:\n",
    "                results[k] = [v]\n",
    "\n",
    "    # Compute mean and standard deviation\n",
    "    print(\"***** Cross-Validation Results *****\")\n",
    "    for k, v in results.items():\n",
    "        skip = True\n",
    "        for m in [\"accuracy\", \"precision\", \"recall\", \"f1-score\"]:\n",
    "            if m in k:\n",
    "                skip = False\n",
    "        if skip:\n",
    "            continue\n",
    "        mean, std, pstd = (\n",
    "            statistics.mean(v),\n",
    "            statistics.stdev(v),\n",
    "            statistics.pstdev(v),\n",
    "        )\n",
    "        print(\n",
    "            k + \":\\n\",\n",
    "            \"Mean:\",\n",
    "            round(mean, 4),\n",
    "            \"\\tStd.:\",\n",
    "            round(std, 4),\n",
    "            \"\\tPStd:\",\n",
    "            round(pstd, 4),\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set hyperparameters\n",
    "\n",
    "class Args:\n",
    "    model = model\n",
    "    output_dir = output_dir\n",
    "    data_dir = data_dir\n",
    "    data_lang = lang\n",
    "    max_len = max_len\n",
    "    lowercase = lowercase\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps\n",
    "    learning_rate = learning_rate\n",
    "    weight_decay = weight_decay\n",
    "    random_weights = random_weights\n",
    "    adam_epsilon = adam_epsilon\n",
    "    max_grad_norm = max_grad_norm\n",
    "    batch_size = batch_size\n",
    "    epochs = epochs\n",
    "    max_steps = max_steps\n",
    "    warmup_steps = warmup_steps\n",
    "    early_stop = early_stop\n",
    "    eval_steps = eval_steps\n",
    "    num_labels = num_labels\n",
    "    seed = seed\n",
    "    save_strategy = save_strategy\n",
    "    load_best_model_at_end = load_best_model_at_end\n",
    "\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "args.train_path = os.path.join(data_dir, \"trial\", \"train\", lang+\".tsv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run training\n",
    "\n",
    "train(args)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "d9c75a10141fb9765b6dd95572dd5e09c2b8655ce454a09c7fa750dc40f7865b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}